{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZohebAbai/Deep-Learning-Projects/blob/master/Tensorflow_Keras/LSTM%20RNN/Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tcSj6H2Qi3E"
   },
   "source": [
    "# LSTM Recurrent Neural Networks used as Generative models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfalutALUE3b"
   },
   "source": [
    "**Load the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "au_lOc3bTnUO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tb4_9by9Ryo4"
   },
   "source": [
    "**Download a free corpus of text that we can use to train text generative models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9bsUCJjSmDG"
   },
   "source": [
    "Download the complete text of *Aliceâ€™s Adventures in Wonderland by Lewis Carroll* in ASCII format (Plain Text UTF-8) for free. Project Gutenberg adds a standard header and footer to each book and this is not part of the original text. Open the file in a text editor and delete the header and footer. Save it and load it as `wonderland.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H90t8pATUKyp"
   },
   "source": [
    "**Load ascii text and covert to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Moln8hUFMaM"
   },
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sofrEHpTCM9"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "At first we will remove the 15 punctuations from the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oV0ZAMGCBr3N"
   },
   "outputs": [],
   "source": [
    "punc_list = ['!', '\"',\"'\",'(',')','*',',','-',':','.',';','?','[',']','_']\n",
    "punc_rem_text =  ''.join(c for c in raw_text if c not in punc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIgT2pheBwOI"
   },
   "source": [
    "We prepare the Data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers. We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "NSl04ZqaT_l1",
    "outputId": "c197b331-85db-4aa8-94c4-987eb096ba3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '0': 2,\n",
       " '3': 3,\n",
       " 'a': 4,\n",
       " 'b': 5,\n",
       " 'c': 6,\n",
       " 'd': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'h': 11,\n",
       " 'i': 12,\n",
       " 'j': 13,\n",
       " 'k': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'q': 20,\n",
       " 'r': 21,\n",
       " 's': 22,\n",
       " 't': 23,\n",
       " 'u': 24,\n",
       " 'v': 25,\n",
       " 'w': 26,\n",
       " 'x': 27,\n",
       " 'y': 28,\n",
       " 'z': 29}"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(punc_rem_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2QPZqwwXFZU"
   },
   "source": [
    "Summarize the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3dPiz7IVWn3D",
    "outputId": "a952302c-085c-4c05-aeed-09a68f108acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  136110\n",
      "Total Vocab:  30\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(punc_rem_text)\n",
    "chars = sorted(list(set(punc_rem_text)))\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8to9ax53pSS9"
   },
   "source": [
    "We split the book into sequences based on end of lines. We don't consider full sentences as the learning is based on character level and few sentenses are too long to introduce lot of sparsity in the model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCK7pseuHJNt"
   },
   "outputs": [],
   "source": [
    "list_of_sen = punc_rem_text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mb3BfsGeZkQx"
   },
   "source": [
    "We convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgtGG_PyNhz5"
   },
   "outputs": [],
   "source": [
    "list_of_int=[]\n",
    "for sen in list_of_sen:\n",
    "  list_of_int.append([char_to_int[c] for c in sen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKHHD6z_QYvZ"
   },
   "source": [
    "We remove the unwanted empty sequences formed due to multiple ends between lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJgHFSBYKLJe"
   },
   "outputs": [],
   "source": [
    "data_before = [x for x in list_of_int if x != []]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RztObozQjwx"
   },
   "source": [
    "Summary of the length of sequences based on end of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCAysroUqYIz",
    "outputId": "db9a497d-b06c-4c20-a8f5-b37d3dc989f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 2, 53.515517936316, 63.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = [len(i) for i in data_before]\n",
    "max(j), min(j), np.mean(j), np.median(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "o8RUiR2efPsa",
    "outputId": "ac8098d1-f2b7-425b-d1ee-6bbb5e92ca98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Probability')"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npKb/C0/MmHES44nJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(np.array(j), density=True, bins=30)\n",
    "plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8NTW_WoPbN3"
   },
   "source": [
    "We combine the sequences to form a final array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW4_EaWi5tQe"
   },
   "outputs": [],
   "source": [
    "dataX_list = [ch for sublist in data_before for ch in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2U0phppRNXv"
   },
   "source": [
    "Prepare the dataset by creating padded sequences for every 63 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZTs5xx4I2OPP",
    "outputId": "d2553b14-0d41-4f38-9807-46e39cdf898f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  130953\n"
     ]
    }
   ],
   "source": [
    "#prepare the dataset of input to output pairs encoded as integers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Use a sequence length of 63 (median)\n",
    "seq_length = 63\n",
    "n_int = len(dataX_list)\n",
    "dataX = []\n",
    "dataY = []\n",
    "count = 0\n",
    "input_seq = []\n",
    "\n",
    "# Run a for loop to fetch each sequence of length 63\n",
    "for i in range(0, n_int, seq_length):\n",
    "  seq_in = dataX_list[i:i + seq_length]\n",
    "  \n",
    "  # # Run through the entire sequence\n",
    "  for j in range(0, len(seq_in)-1):\n",
    "    count += 1\n",
    "    \n",
    "    # Create sub-sequences from length 1 to 100\n",
    "    in_seq = seq_in[:j+1]\n",
    "    out_seq = seq_in[j+1]\n",
    "    \n",
    "    # Append the sub-sequences together\n",
    "    input_seq.append([int_ for int_ in in_seq])\n",
    "    dataY.append(out_seq)\n",
    "\n",
    "# Pad all the sequences to length 100, use pre-padding    \n",
    "dataX = pad_sequences(input_seq, maxlen=seq_length, padding='pre')\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "2EWcIYzfZubV",
    "outputId": "89523aac-5aaa-4205-be40-9d4a7ccccca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  4, 15], dtype=int32), 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX[1], dataY[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "771tVmlNapZP"
   },
   "source": [
    "* First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "* Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "* Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUENUc70Z-SA"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAroZ4VlbInW"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "N1Tcvl4ma8jv",
    "outputId": "c56b5981-1f08-4929-f98a-f56e20319f36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 02:55:45.176415 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 02:55:45.216488 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 02:55:45.225648 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 02:55:45.456321 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0728 02:55:45.471931 140255759828864 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0728 02:55:46.137270 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 02:55:46.168350 140255759828864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), dropout=0.1, return_sequences=True))\n",
    "model.add(LSTM(256, dropout=0.1))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlW69F-wbrt6"
   },
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence. You will see different results because of the stochastic nature of the model, and because it is hard to fix the random seed for LSTM models to get 100% reproducible results. This is not a concern for this generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aEdT6jzEbpHw",
    "outputId": "e22692d8-1570-4dd6-ceff-be1acd65b29f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 02:55:46.353408 140255759828864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "130953/130953 [==============================] - 304s 2ms/step - loss: 2.7235\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.72352, saving model to model.h5\n",
      "Epoch 2/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 2.4065\n",
      "\n",
      "Epoch 00002: loss improved from 2.72352 to 2.40651, saving model to model.h5\n",
      "Epoch 3/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 2.2154\n",
      "\n",
      "Epoch 00003: loss improved from 2.40651 to 2.21544, saving model to model.h5\n",
      "Epoch 4/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 2.0974\n",
      "\n",
      "Epoch 00004: loss improved from 2.21544 to 2.09736, saving model to model.h5\n",
      "Epoch 5/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 2.0125\n",
      "\n",
      "Epoch 00005: loss improved from 2.09736 to 2.01250, saving model to model.h5\n",
      "Epoch 6/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.9458\n",
      "\n",
      "Epoch 00006: loss improved from 2.01250 to 1.94583, saving model to model.h5\n",
      "Epoch 7/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.8830\n",
      "\n",
      "Epoch 00007: loss improved from 1.94583 to 1.88297, saving model to model.h5\n",
      "Epoch 8/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.8331\n",
      "\n",
      "Epoch 00008: loss improved from 1.88297 to 1.83315, saving model to model.h5\n",
      "Epoch 9/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.7879\n",
      "\n",
      "Epoch 00009: loss improved from 1.83315 to 1.78786, saving model to model.h5\n",
      "Epoch 10/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.7469\n",
      "\n",
      "Epoch 00010: loss improved from 1.78786 to 1.74693, saving model to model.h5\n",
      "Epoch 11/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.7052\n",
      "\n",
      "Epoch 00011: loss improved from 1.74693 to 1.70518, saving model to model.h5\n",
      "Epoch 12/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.6688\n",
      "\n",
      "Epoch 00012: loss improved from 1.70518 to 1.66876, saving model to model.h5\n",
      "Epoch 13/100\n",
      "130953/130953 [==============================] - 303s 2ms/step - loss: 1.6350\n",
      "\n",
      "Epoch 00013: loss improved from 1.66876 to 1.63502, saving model to model.h5\n",
      "Epoch 14/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.6045\n",
      "\n",
      "Epoch 00014: loss improved from 1.63502 to 1.60453, saving model to model.h5\n",
      "Epoch 15/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.5785\n",
      "\n",
      "Epoch 00015: loss improved from 1.60453 to 1.57846, saving model to model.h5\n",
      "Epoch 16/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.5449\n",
      "\n",
      "Epoch 00016: loss improved from 1.57846 to 1.54491, saving model to model.h5\n",
      "Epoch 17/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.5173\n",
      "\n",
      "Epoch 00017: loss improved from 1.54491 to 1.51730, saving model to model.h5\n",
      "Epoch 18/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.4876\n",
      "\n",
      "Epoch 00018: loss improved from 1.51730 to 1.48757, saving model to model.h5\n",
      "Epoch 19/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.4649\n",
      "\n",
      "Epoch 00019: loss improved from 1.48757 to 1.46495, saving model to model.h5\n",
      "Epoch 20/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.4401\n",
      "\n",
      "Epoch 00020: loss improved from 1.46495 to 1.44012, saving model to model.h5\n",
      "Epoch 21/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.4176\n",
      "\n",
      "Epoch 00021: loss improved from 1.44012 to 1.41757, saving model to model.h5\n",
      "Epoch 22/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.3969\n",
      "\n",
      "Epoch 00022: loss improved from 1.41757 to 1.39691, saving model to model.h5\n",
      "Epoch 23/100\n",
      "130953/130953 [==============================] - 302s 2ms/step - loss: 1.3740\n",
      "\n",
      "Epoch 00023: loss improved from 1.39691 to 1.37397, saving model to model.h5\n",
      "Epoch 24/100\n",
      "130953/130953 [==============================] - 302s 2ms/step - loss: 1.3544\n",
      "\n",
      "Epoch 00024: loss improved from 1.37397 to 1.35442, saving model to model.h5\n",
      "Epoch 25/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.3374\n",
      "\n",
      "Epoch 00025: loss improved from 1.35442 to 1.33745, saving model to model.h5\n",
      "Epoch 26/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.3144\n",
      "\n",
      "Epoch 00026: loss improved from 1.33745 to 1.31436, saving model to model.h5\n",
      "Epoch 27/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.3004\n",
      "\n",
      "Epoch 00027: loss improved from 1.31436 to 1.30043, saving model to model.h5\n",
      "Epoch 28/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.2822\n",
      "\n",
      "Epoch 00028: loss improved from 1.30043 to 1.28221, saving model to model.h5\n",
      "Epoch 29/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.2621\n",
      "\n",
      "Epoch 00029: loss improved from 1.28221 to 1.26209, saving model to model.h5\n",
      "Epoch 30/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.2456\n",
      "\n",
      "Epoch 00030: loss improved from 1.26209 to 1.24563, saving model to model.h5\n",
      "Epoch 31/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.2292\n",
      "\n",
      "Epoch 00031: loss improved from 1.24563 to 1.22915, saving model to model.h5\n",
      "Epoch 32/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.2166\n",
      "\n",
      "Epoch 00032: loss improved from 1.22915 to 1.21665, saving model to model.h5\n",
      "Epoch 33/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.2152\n",
      "\n",
      "Epoch 00033: loss improved from 1.21665 to 1.21519, saving model to model.h5\n",
      "Epoch 34/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.1959\n",
      "\n",
      "Epoch 00034: loss improved from 1.21519 to 1.19587, saving model to model.h5\n",
      "Epoch 35/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.1739\n",
      "\n",
      "Epoch 00035: loss improved from 1.19587 to 1.17385, saving model to model.h5\n",
      "Epoch 36/100\n",
      "130953/130953 [==============================] - 299s 2ms/step - loss: 1.1646\n",
      "\n",
      "Epoch 00036: loss improved from 1.17385 to 1.16464, saving model to model.h5\n",
      "Epoch 37/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.1525\n",
      "\n",
      "Epoch 00037: loss improved from 1.16464 to 1.15246, saving model to model.h5\n",
      "Epoch 38/100\n",
      "130953/130953 [==============================] - 298s 2ms/step - loss: 1.1358\n",
      "\n",
      "Epoch 00038: loss improved from 1.15246 to 1.13582, saving model to model.h5\n",
      "Epoch 39/100\n",
      "130953/130953 [==============================] - 302s 2ms/step - loss: 1.1275\n",
      "\n",
      "Epoch 00039: loss improved from 1.13582 to 1.12747, saving model to model.h5\n",
      "Epoch 40/100\n",
      "130953/130953 [==============================] - 302s 2ms/step - loss: 1.1180\n",
      "\n",
      "Epoch 00040: loss improved from 1.12747 to 1.11799, saving model to model.h5\n",
      "Epoch 41/100\n",
      "130953/130953 [==============================] - 303s 2ms/step - loss: 1.1028\n",
      "\n",
      "Epoch 00041: loss improved from 1.11799 to 1.10275, saving model to model.h5\n",
      "Epoch 42/100\n",
      "130953/130953 [==============================] - 301s 2ms/step - loss: 1.0995\n",
      "\n",
      "Epoch 00042: loss improved from 1.10275 to 1.09953, saving model to model.h5\n",
      "Epoch 43/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.0787\n",
      "\n",
      "Epoch 00043: loss improved from 1.09953 to 1.07872, saving model to model.h5\n",
      "Epoch 44/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.0742\n",
      "\n",
      "Epoch 00044: loss improved from 1.07872 to 1.07423, saving model to model.h5\n",
      "Epoch 45/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.0667\n",
      "\n",
      "Epoch 00045: loss improved from 1.07423 to 1.06668, saving model to model.h5\n",
      "Epoch 46/100\n",
      "130953/130953 [==============================] - 300s 2ms/step - loss: 1.0526\n",
      "\n",
      "Epoch 00046: loss improved from 1.06668 to 1.05258, saving model to model.h5\n",
      "Epoch 47/100\n",
      "130953/130953 [==============================] - 295s 2ms/step - loss: 1.0492\n",
      "\n",
      "Epoch 00047: loss improved from 1.05258 to 1.04923, saving model to model.h5\n",
      "Epoch 48/100\n",
      "130953/130953 [==============================] - 296s 2ms/step - loss: 1.0371\n",
      "\n",
      "Epoch 00048: loss improved from 1.04923 to 1.03707, saving model to model.h5\n",
      "Epoch 49/100\n",
      "130953/130953 [==============================] - 296s 2ms/step - loss: 1.0310\n",
      "\n",
      "Epoch 00049: loss improved from 1.03707 to 1.03104, saving model to model.h5\n",
      "Epoch 50/100\n",
      "130953/130953 [==============================] - 297s 2ms/step - loss: 1.0198\n",
      "\n",
      "Epoch 00050: loss improved from 1.03104 to 1.01979, saving model to model.h5\n",
      "Epoch 51/100\n",
      "130953/130953 [==============================] - 297s 2ms/step - loss: 1.0123\n",
      "\n",
      "Epoch 00051: loss improved from 1.01979 to 1.01227, saving model to model.h5\n",
      "Epoch 52/100\n",
      "130953/130953 [==============================] - 297s 2ms/step - loss: 1.0051\n",
      "\n",
      "Epoch 00052: loss improved from 1.01227 to 1.00512, saving model to model.h5\n",
      "Epoch 53/100\n",
      "130953/130953 [==============================] - 298s 2ms/step - loss: 0.9975\n",
      "\n",
      "Epoch 00053: loss improved from 1.00512 to 0.99753, saving model to model.h5\n",
      "Epoch 54/100\n",
      "130953/130953 [==============================] - 297s 2ms/step - loss: 0.9883\n",
      "\n",
      "Epoch 00054: loss improved from 0.99753 to 0.98827, saving model to model.h5\n",
      "Epoch 55/100\n",
      "130953/130953 [==============================] - 295s 2ms/step - loss: 0.9867\n",
      "\n",
      "Epoch 00055: loss improved from 0.98827 to 0.98671, saving model to model.h5\n",
      "Epoch 56/100\n",
      "130953/130953 [==============================] - 294s 2ms/step - loss: 0.9746\n",
      "\n",
      "Epoch 00056: loss improved from 0.98671 to 0.97460, saving model to model.h5\n",
      "Epoch 57/100\n",
      "130953/130953 [==============================] - 298s 2ms/step - loss: 0.9713\n",
      "\n",
      "Epoch 00057: loss improved from 0.97460 to 0.97132, saving model to model.h5\n",
      "Epoch 58/100\n",
      "130953/130953 [==============================] - 296s 2ms/step - loss: 0.9683\n",
      "\n",
      "Epoch 00058: loss improved from 0.97132 to 0.96831, saving model to model.h5\n",
      "Epoch 59/100\n",
      "130953/130953 [==============================] - 298s 2ms/step - loss: 0.9529\n",
      "\n",
      "Epoch 00059: loss improved from 0.96831 to 0.95290, saving model to model.h5\n",
      "Epoch 60/100\n",
      " 91776/130953 [====================>.........] - ETA: 1:29 - loss: 0.9369Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "checkpoint = ModelCheckpoint(\"model.h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Fit the Model to data\n",
    "model.fit(X, y, epochs=100, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PN4_OaRycmbx"
   },
   "source": [
    "## Generating Text with the trained LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSPBHJ2Icpqh"
   },
   "outputs": [],
   "source": [
    "# load the trained network\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE2SfE_3eNn1"
   },
   "source": [
    "When preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZZpVbiFeNQ2"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHmvu77zeYPc"
   },
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 500 characters in length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "r-gx2q_oeegF",
    "outputId": "0f5c35ce-b6dc-40c0-dada-c3e0ac7b0605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r if \"\n",
      "r eheaned iersenf dillgh of tuist sosrttoetha got to the thould ruarren i hrt the wroow littlebusw and temsvedistell the mike anotely iengral eo loto anices dlnsstear it taid the cames cheak followg of near the toodiersoo and alice lade oife thak drnesuion yhat took iis face and what if said to the tuiek and tuuetge satted iishast yordering toundny sfrulngoong sand yiat firtt anl touw of teyt tucceumat he said tu tay toeezing anicerulzes shruted the fouterst the whought anice tuondling hir vor f\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "import sys\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = list(dataX[start])\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnFO60MWC-oA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Assignment 6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
